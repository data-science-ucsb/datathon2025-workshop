{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5gBU5nWURuE"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN example\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, learning_rate=0.001, batch_size=32):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 48, kernel_size=3) # 1st convolution\n",
        "        self.conv2 = nn.Conv2d(48, 48, kernel_size=3) # 2nd convolution\n",
        "        self.pool = nn.MaxPool2d(3, 3) # Pooling - condensing down to weighted sum of a region\n",
        "        self.fc1 = nn.Linear(48 * 4 * 4, 144) # Adjusted output size\n",
        "        self.fc2 = nn.Linear(144, 48) # Corrected input size to match the output of self.fc1\n",
        "        self.out = nn.Linear(48, 24)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.reshape(-1, 48 * 4 * 4) # Corrected reshaping based on the calculated dimensions\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.softmax(self.out(x), dim=1)"
      ],
      "metadata": {
        "id": "57Itnex0Uyuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 32\n",
        "    num_epochs = 5\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # Data transformation: Resize to 48x48 to match the CNN's expected input\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((48, 48)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load MNIST dataset (grayscale images)\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = CNN(learning_rate=learning_rate, batch_size=batch_size)\n",
        "    # Adjust the final layer to match MNIST's 10 classes:\n",
        "    model.out = nn.Linear(48, 10)\n",
        "\n",
        "    # Optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # For training, we need raw logits (since CrossEntropyLoss applies softmax internally).\n",
        "    # Thus, we define a forward function without the softmax:\n",
        "    def forward_without_softmax(x):\n",
        "        x = F.relu(model.conv1(x))\n",
        "        x = model.pool(x)\n",
        "        x = F.relu(model.conv2(x))\n",
        "        x = model.pool(x)\n",
        "        x = x.view(-1, 48 * 4 * 4)\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = F.relu(model.fc2(x))\n",
        "        return model.out(x)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad() # zero out gradient after each iteration\n",
        "            outputs = forward_without_softmax(images) # raw logits\n",
        "            loss = criterion(outputs, labels) # calculate gradient\n",
        "            loss.backward() # use loss gradient\n",
        "            optimizer.step() # re-eval model and return loss\n",
        "            running_loss += loss.item()\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "        print(f\"Epoch [{epoch+1}] Average Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = forward_without_softmax(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ],
      "metadata": {
        "id": "qg8TKp6hZrgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenCV functions\n",
        "# Edge detection\n",
        "def show_canny(img):\n",
        "    canny = cv.Canny(cv.imread(img), 100, 200)\n",
        "    cv.imshow(img, canny)\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyWindow(img)\n",
        "\n",
        "# Show image\n",
        "def show_img(img):\n",
        "    cv.imshow(img, cv.imread(img))\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyWindow(img)\n",
        "\n",
        "# Grayscale\n",
        "def show_grayscale(img):\n",
        "    gray = cv.cvtColor(cv.imread(img), cv.COLOR_BGR2GRAY)\n",
        "    cv.imshow(img, gray)\n",
        "    cv.waitKey(0)\n",
        "    cv.destroyWindow(img)\n",
        "\n",
        "def show_video():\n",
        "    import cv2\n",
        "\n",
        "    # Open video file or webcam (0 for default webcam)\n",
        "    cap = cv2.VideoCapture(\"video.mp4\")  # Change to 0 for webcam\n",
        "\n",
        "    # Get the original frame rate (FPS)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    print(f\"Original FPS: {fps}\")\n",
        "\n",
        "    # Set a delay between frames to match the original FPS\n",
        "    frame_delay = int(1000 / fps)  # Convert FPS to milliseconds\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(\"End of video stream or cannot fetch frame.\")\n",
        "            break\n",
        "\n",
        "        # Show the frame\n",
        "        cv2.imshow(\"Video\", frame)\n",
        "\n",
        "        # Wait for 'q' key or for the frame delay\n",
        "        if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "VXUyESrkafwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenCV to PIL Image\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def opencv_to_pil(opencv_image):\n",
        "    # Convert the OpenCV image (BGR) to RGB\n",
        "    rgb_image = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert the RGB image to a PIL Image object\n",
        "    pil_image = Image.fromarray(rgb_image)\n",
        "\n",
        "    return pil_image\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'image_path' is the path to your image file\n",
        "image_path = 'path/to/your/image.jpg'\n",
        "opencv_image = cv2.imread(image_path)\n",
        "\n",
        "if opencv_image is not None:\n",
        "    pil_image = opencv_to_pil(opencv_image)\n",
        "\n",
        "    # You can now work with the PIL image, e.g., display it\n",
        "    pil_image.show()\n",
        "\n",
        "    # Or save it\n",
        "    pil_image.save(\"path/to/save/pil_image.png\")\n",
        "else:\n",
        "    print(f\"Error: Could not open or read the image from {image_path}\")"
      ],
      "metadata": {
        "id": "hh8MIo2KK5wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image using OpenCV\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image is None:\n",
        "    raise Exception(\"Could not open or find the image\")\n",
        "\n",
        "# Convert the image to a NumPy array\n",
        "numpy_image = np.array(image)\n",
        "\n",
        "# Print the shape of the NumPy array (height, width, channels)\n",
        "print(numpy_image.shape)\n",
        "\n",
        "# Print the data type of the NumPy array\n",
        "print(numpy_image.dtype)"
      ],
      "metadata": {
        "id": "OUcRKbTuLQQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case study\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained ResNet18 model and set it to evaluation mode\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define the image preprocessing steps expected by ResNet18\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),               # Convert OpenCV image (NumPy array) to PIL image\n",
        "    transforms.Resize(256),                # Resize the image so the shorter side is 256 pixels\n",
        "    transforms.CenterCrop(224),            # Crop the center 224x224 portion\n",
        "    transforms.ToTensor(),                 # Convert PIL image to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize using ImageNet's mean and std\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the class names from the file\n",
        "with open(\"imagenet_classes.txt\") as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Start video capture using OpenCV (0 is usually the default webcam)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert frame from BGR (OpenCV default) to RGB\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Preprocess the frame and add a batch dimension\n",
        "    input_tensor = preprocess(rgb_frame)\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "    # Run the model in inference mode\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "\n",
        "    # Find the predicted class index and corresponding label\n",
        "    _, predicted_idx = torch.max(output, 1)\n",
        "    predicted_label = classes[predicted_idx]\n",
        "\n",
        "    # Overlay the predicted label on the frame\n",
        "    cv2.putText(frame, predicted_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Display the annotated frame\n",
        "    cv2.imshow('Real-Time Classification', frame)\n",
        "\n",
        "    # Press 'q' to exit the loop\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "ozBQDPtgKnJb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}